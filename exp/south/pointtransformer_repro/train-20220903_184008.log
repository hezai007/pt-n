[2022-09-03 18:40:09,595 INFO train_south.py line 120 27002] arch: pointtransformer_seg_repro
base_lr: 0.5
batch_size: 4
batch_size_test: 4
batch_size_val: 4
classes: 3
data_name: south
data_root: dataset/south/trainval_fullarea
dist_backend: nccl
dist_url: tcp://localhost:8888
distributed: False
drop_rate: 0.5
epochs: 150
eval_freq: 1
evaluate: True
fea_dim: 6
ignore_label: 255
loop: 30
manual_seed: 7777
model_path: None
momentum: 0.9
multiplier: 0.1
multiprocessing_distributed: False
names_path: data/south/south_names.txt
ngpus_per_node: 1
print_freq: 1
rank: 0
resume: None
save_folder: None
save_freq: 1
save_path: exp/south/pointtransformer_repro
split: val
start_epoch: 0
step_epoch: 30
sync_bn: False
test_area: 5
test_gpu: [4]
test_list: dataset/south/list/val.txt
test_list_full: dataset/south/list/val_full.txt
test_workers: 4
train_gpu: [4]
use_xyz: True
voxel_max: 80000
voxel_size: 0.04
weight: None
weight_decay: 0.0001
workers: 16
world_size: 1
[2022-09-03 18:40:09,595 INFO train_south.py line 121 27002] => creating model ...
[2022-09-03 18:40:09,595 INFO train_south.py line 122 27002] Classes: 3
[2022-09-03 18:40:09,595 INFO train_south.py line 123 27002] PointTransformerSeg(
  (enc1): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=6, out_features=32, bias=False)
      (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=32, out_features=32, bias=False)
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=32, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=32, out_features=4, bias=True)
          (3): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=4, out_features=4, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=32, out_features=32, bias=False)
      (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc2): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=35, out_features=64, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=64, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=64, out_features=8, bias=True)
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=8, out_features=8, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=64, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=64, out_features=8, bias=True)
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=8, out_features=8, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc3): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=67, out_features=128, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=128, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=128, out_features=16, bias=True)
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=16, out_features=16, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=128, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=128, out_features=16, bias=True)
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=16, out_features=16, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=128, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=128, out_features=16, bias=True)
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=16, out_features=16, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc4): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=131, out_features=256, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=256, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=32, out_features=32, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=256, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=32, out_features=32, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=256, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=32, out_features=32, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=256, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=32, out_features=32, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=256, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=32, out_features=32, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc5): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=259, out_features=512, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=512, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=64, bias=True)
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=64, out_features=64, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=512, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=64, bias=True)
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=64, out_features=64, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (dec5): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=512, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=512, out_features=64, bias=True)
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=64, out_features=64, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (dec4): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=256, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=32, out_features=32, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (dec3): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=128, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=128, out_features=16, bias=True)
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=16, out_features=16, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (dec2): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=64, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=64, out_features=8, bias=True)
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=8, out_features=8, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (dec1): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=32, out_features=32, bias=False)
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (linear_p): Sequential(
          (0): Linear(in_features=3, out_features=3, bias=True)
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=32, bias=True)
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=32, out_features=4, bias=True)
          (3): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Linear(in_features=4, out_features=4, bias=True)
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=32, out_features=32, bias=False)
      (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (cls): Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Linear(in_features=32, out_features=3, bias=True)
  )
)
[2022-09-03 18:40:13,400 INFO train_south.py line 169 27002] train_data samples: '210'
Totally 7 samples in train set.
Totally 2 samples in val set.
Totally 7 samples in train set.
Totally 2 samples in val set.
/home/wanghe/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)
  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)
[2022-09-03 18:40:35,837 INFO train_south.py line 277 27002] Epoch: [1/150][1/52] Data 19.725 (19.725) Batch 22.433 (22.433) Remain 48:35:58 Loss 1.1198 Accuracy 0.3336.
[2022-09-03 18:40:46,642 INFO train_south.py line 277 27002] Epoch: [1/150][2/52] Data 9.225 (14.475) Batch 10.805 (16.619) Remain 35:59:57 Loss 1.1116 Accuracy 0.3331.
[2022-09-03 18:40:48,206 INFO train_south.py line 277 27002] Epoch: [1/150][3/52] Data 0.001 (9.650) Batch 1.564 (11.601) Remain 25:07:32 Loss 1.1085 Accuracy 0.3342.
[2022-09-03 18:41:18,167 INFO train_south.py line 277 27002] Epoch: [1/150][4/52] Data 28.329 (14.320) Batch 29.961 (16.191) Remain 35:03:44 Loss 1.1056 Accuracy 0.3324.
[2022-09-03 18:41:19,720 INFO train_south.py line 277 27002] Epoch: [1/150][5/52] Data 0.002 (11.456) Batch 1.553 (13.263) Remain 28:43:07 Loss 1.1035 Accuracy 0.3322.
[2022-09-03 18:41:21,271 INFO train_south.py line 277 27002] Epoch: [1/150][6/52] Data 0.001 (9.547) Batch 1.551 (11.311) Remain 24:29:19 Loss 1.1052 Accuracy 0.3330.
[2022-09-03 18:41:22,820 INFO train_south.py line 277 27002] Epoch: [1/150][7/52] Data 0.002 (8.183) Batch 1.549 (9.917) Remain 21:28:00 Loss 1.1017 Accuracy 0.3346.
[2022-09-03 18:41:24,366 INFO train_south.py line 277 27002] Epoch: [1/150][8/52] Data 0.001 (7.161) Batch 1.547 (8.870) Remain 19:11:57 Loss 1.1012 Accuracy 0.3332.
[2022-09-03 18:41:25,985 INFO train_south.py line 277 27002] Epoch: [1/150][9/52] Data 0.001 (6.365) Batch 1.619 (8.065) Remain 17:27:11 Loss 1.1017 Accuracy 0.3351.
[2022-09-03 18:41:27,538 INFO train_south.py line 277 27002] Epoch: [1/150][10/52] Data 0.001 (5.729) Batch 1.553 (7.413) Remain 16:02:31 Loss 1.1003 Accuracy 0.3325.
[2022-09-03 18:41:29,130 INFO train_south.py line 277 27002] Epoch: [1/150][11/52] Data 0.001 (5.208) Batch 1.592 (6.884) Remain 14:53:41 Loss 1.0998 Accuracy 0.3342.
[2022-09-03 18:41:30,679 INFO train_south.py line 277 27002] Epoch: [1/150][12/52] Data 0.001 (4.774) Batch 1.550 (6.440) Remain 13:55:52 Loss 1.1007 Accuracy 0.3330.
[2022-09-03 18:41:32,232 INFO train_south.py line 277 27002] Epoch: [1/150][13/52] Data 0.001 (4.407) Batch 1.553 (6.064) Remain 13:06:58 Loss 1.1000 Accuracy 0.3335.
[2022-09-03 18:41:33,791 INFO train_south.py line 277 27002] Epoch: [1/150][14/52] Data 0.001 (4.092) Batch 1.558 (5.742) Remain 12:25:06 Loss 1.0997 Accuracy 0.3341.
[2022-09-03 18:41:35,350 INFO train_south.py line 277 27002] Epoch: [1/150][15/52] Data 0.002 (3.820) Batch 1.560 (5.463) Remain 11:48:50 Loss 1.1001 Accuracy 0.3340.
[2022-09-03 18:41:36,905 INFO train_south.py line 277 27002] Epoch: [1/150][16/52] Data 0.001 (3.581) Batch 1.554 (5.219) Remain 11:17:03 Loss 1.0995 Accuracy 0.3329.
[2022-09-03 18:41:38,495 INFO train_south.py line 277 27002] Epoch: [1/150][17/52] Data 0.001 (3.370) Batch 1.590 (5.005) Remain 10:49:16 Loss 1.0993 Accuracy 0.3339.
[2022-09-03 18:42:08,930 INFO train_south.py line 277 27002] Epoch: [1/150][18/52] Data 28.846 (4.786) Batch 30.435 (6.418) Remain 13:52:26 Loss 1.0998 Accuracy 0.3338.
[2022-09-03 18:42:10,485 INFO train_south.py line 277 27002] Epoch: [1/150][19/52] Data 0.001 (4.534) Batch 1.555 (6.162) Remain 13:19:08 Loss 1.0993 Accuracy 0.3338.
[2022-09-03 18:42:12,045 INFO train_south.py line 277 27002] Epoch: [1/150][20/52] Data 0.001 (4.307) Batch 1.560 (5.932) Remain 12:49:11 Loss 1.0993 Accuracy 0.3330.
[2022-09-03 18:42:13,609 INFO train_south.py line 277 27002] Epoch: [1/150][21/52] Data 0.002 (4.102) Batch 1.564 (5.724) Remain 12:22:07 Loss 1.0995 Accuracy 0.3342.
[2022-09-03 18:42:15,168 INFO train_south.py line 277 27002] Epoch: [1/150][22/52] Data 0.007 (3.916) Batch 1.559 (5.535) Remain 11:57:29 Loss 1.0993 Accuracy 0.3322.
[2022-09-03 18:42:16,732 INFO train_south.py line 277 27002] Epoch: [1/150][23/52] Data 0.001 (3.746) Batch 1.564 (5.362) Remain 11:35:01 Loss 1.0992 Accuracy 0.3331.
[2022-09-03 18:42:18,333 INFO train_south.py line 277 27002] Epoch: [1/150][24/52] Data 0.002 (3.590) Batch 1.601 (5.205) Remain 11:14:37 Loss 1.0994 Accuracy 0.3327.
[2022-09-03 18:42:36,687 INFO train_south.py line 277 27002] Epoch: [1/150][25/52] Data 16.737 (4.116) Batch 18.354 (5.731) Remain 12:22:41 Loss 1.0990 Accuracy 0.3339.
[2022-09-03 18:42:38,242 INFO train_south.py line 277 27002] Epoch: [1/150][26/52] Data 0.001 (3.958) Batch 1.556 (5.571) Remain 12:01:46 Loss 1.0991 Accuracy 0.3315.
[2022-09-03 18:42:39,815 INFO train_south.py line 277 27002] Epoch: [1/150][27/52] Data 0.001 (3.811) Batch 1.573 (5.423) Remain 11:42:30 Loss 1.0991 Accuracy 0.3350.
[2022-09-03 18:42:41,366 INFO train_south.py line 277 27002] Epoch: [1/150][28/52] Data 0.001 (3.675) Batch 1.551 (5.284) Remain 11:24:30 Loss 1.0989 Accuracy 0.3337.
[2022-09-03 18:42:42,920 INFO train_south.py line 277 27002] Epoch: [1/150][29/52] Data 0.002 (3.548) Batch 1.554 (5.156) Remain 11:07:45 Loss 1.0988 Accuracy 0.3334.
[2022-09-03 18:42:44,492 INFO train_south.py line 277 27002] Epoch: [1/150][30/52] Data 0.002 (3.430) Batch 1.572 (5.036) Remain 10:52:11 Loss 1.0991 Accuracy 0.3323.
[2022-09-03 18:42:46,043 INFO train_south.py line 277 27002] Epoch: [1/150][31/52] Data 0.001 (3.319) Batch 1.552 (4.924) Remain 10:37:33 Loss 1.0989 Accuracy 0.3339.
[2022-09-03 18:42:47,598 INFO train_south.py line 277 27002] Epoch: [1/150][32/52] Data 0.001 (3.216) Batch 1.555 (4.819) Remain 10:23:50 Loss 1.0988 Accuracy 0.3355.
[2022-09-03 18:42:49,157 INFO train_south.py line 277 27002] Epoch: [1/150][33/52] Data 0.001 (3.118) Batch 1.559 (4.720) Remain 10:10:58 Loss 1.0991 Accuracy 0.3322.
[2022-09-03 18:42:50,767 INFO train_south.py line 277 27002] Epoch: [1/150][34/52] Data 0.001 (3.027) Batch 1.610 (4.628) Remain 09:59:03 Loss 1.0989 Accuracy 0.3323.
[2022-09-03 18:42:52,325 INFO train_south.py line 277 27002] Epoch: [1/150][35/52] Data 0.001 (2.940) Batch 1.559 (4.541) Remain 09:47:37 Loss 1.0988 Accuracy 0.3344.
[2022-09-03 18:42:53,953 INFO train_south.py line 277 27002] Epoch: [1/150][36/52] Data 0.002 (2.859) Batch 1.627 (4.460) Remain 09:37:05 Loss 1.0989 Accuracy 0.3327.
[2022-09-03 18:42:55,509 INFO train_south.py line 277 27002] Epoch: [1/150][37/52] Data 0.001 (2.781) Batch 1.557 (4.381) Remain 09:26:51 Loss 1.0988 Accuracy 0.3336.
[2022-09-03 18:42:57,103 INFO train_south.py line 277 27002] Epoch: [1/150][38/52] Data 0.002 (2.708) Batch 1.594 (4.308) Remain 09:17:17 Loss 1.0988 Accuracy 0.3337.
[2022-09-03 18:42:58,662 INFO train_south.py line 277 27002] Epoch: [1/150][39/52] Data 0.002 (2.639) Batch 1.558 (4.237) Remain 09:08:06 Loss 1.0988 Accuracy 0.3341.
[2022-09-03 18:43:00,221 INFO train_south.py line 277 27002] Epoch: [1/150][40/52] Data 0.001 (2.573) Batch 1.559 (4.170) Remain 08:59:22 Loss 1.0987 Accuracy 0.3340.
[2022-09-03 18:43:01,778 INFO train_south.py line 277 27002] Epoch: [1/150][41/52] Data 0.002 (2.510) Batch 1.558 (4.107) Remain 08:51:03 Loss 1.0987 Accuracy 0.3338.
[2022-09-03 18:43:03,332 INFO train_south.py line 277 27002] Epoch: [1/150][42/52] Data 0.002 (2.450) Batch 1.554 (4.046) Remain 08:43:08 Loss 1.0988 Accuracy 0.3336.
[2022-09-03 18:43:04,887 INFO train_south.py line 277 27002] Epoch: [1/150][43/52] Data 0.001 (2.394) Batch 1.555 (3.988) Remain 08:35:34 Loss 1.0987 Accuracy 0.3334.
[2022-09-03 18:43:06,443 INFO train_south.py line 277 27002] Epoch: [1/150][44/52] Data 0.001 (2.339) Batch 1.555 (3.933) Remain 08:28:22 Loss 1.0988 Accuracy 0.3324.
[2022-09-03 18:43:08,000 INFO train_south.py line 277 27002] Epoch: [1/150][45/52] Data 0.002 (2.287) Batch 1.557 (3.880) Remain 08:21:28 Loss 1.0988 Accuracy 0.3328.
[2022-09-03 18:43:09,559 INFO train_south.py line 277 27002] Epoch: [1/150][46/52] Data 0.002 (2.238) Batch 1.559 (3.829) Remain 08:14:53 Loss 1.0987 Accuracy 0.3331.
[2022-09-03 18:43:11,115 INFO train_south.py line 277 27002] Epoch: [1/150][47/52] Data 0.001 (2.190) Batch 1.556 (3.781) Remain 08:08:34 Loss 1.0987 Accuracy 0.3327.
[2022-09-03 18:43:12,671 INFO train_south.py line 277 27002] Epoch: [1/150][48/52] Data 0.001 (2.144) Batch 1.556 (3.735) Remain 08:02:31 Loss 1.0987 Accuracy 0.3336.
[2022-09-03 18:43:14,228 INFO train_south.py line 277 27002] Epoch: [1/150][49/52] Data 0.001 (2.101) Batch 1.557 (3.690) Remain 07:56:43 Loss 1.0987 Accuracy 0.3339.
[2022-09-03 18:43:15,785 INFO train_south.py line 277 27002] Epoch: [1/150][50/52] Data 0.001 (2.059) Batch 1.557 (3.648) Remain 07:51:09 Loss 1.0987 Accuracy 0.3340.
[2022-09-03 18:43:17,342 INFO train_south.py line 277 27002] Epoch: [1/150][51/52] Data 0.001 (2.018) Batch 1.556 (3.607) Remain 07:45:47 Loss 1.0987 Accuracy 0.3335.
[2022-09-03 18:43:18,897 INFO train_south.py line 277 27002] Epoch: [1/150][52/52] Data 0.001 (1.980) Batch 1.556 (3.567) Remain 07:40:38 Loss 1.0987 Accuracy 0.3332.
[2022-09-03 18:43:19,110 INFO train_south.py line 299 27002] Train result at epoch [1/150]: mIoU/mAcc/allAcc 0.1979/0.3334/0.3334.
[2022-09-03 18:43:19,111 INFO train_south.py line 305 27002] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[2022-09-03 18:44:07,661 INFO train_south.py line 344 27002] Test: [1/1] Data 9.142 (9.142) Batch 48.546 (48.546) Loss 1.0987 (1.0987) Accuracy 0.3345.
[2022-09-03 18:44:07,843 INFO train_south.py line 361 27002] Val result: mIoU/mAcc/allAcc 0.1644/0.3342/0.3345.
[2022-09-03 18:44:07,843 INFO train_south.py line 363 27002] Class_0 Result: iou/accuracy 0.0564/0.0635.
[2022-09-03 18:44:07,843 INFO train_south.py line 363 27002] Class_1 Result: iou/accuracy 0.3012/0.7535.
[2022-09-03 18:44:07,843 INFO train_south.py line 363 27002] Class_2 Result: iou/accuracy 0.1356/0.1856.
[2022-09-03 18:44:07,844 INFO train_south.py line 364 27002] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2022-09-03 18:44:07,844 INFO train_south.py line 215 27002] Saving checkpoint to: exp/south/pointtransformer_repro/model/model_last.pth
[2022-09-03 18:44:08,066 INFO train_south.py line 219 27002] Best validation mIoU updated to: 0.1644
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[2022-09-03 18:44:35,005 INFO train_south.py line 277 27002] Epoch: [2/150][1/52] Data 25.281 (25.281) Batch 26.888 (26.888) Remain 57:51:39 Loss 1.0987 Accuracy 0.3336.
[2022-09-03 18:45:03,051 INFO train_south.py line 277 27002] Epoch: [2/150][2/52] Data 26.441 (25.861) Batch 28.047 (27.467) Remain 59:06:01 Loss 1.0987 Accuracy 0.3329.
[2022-09-03 18:45:04,682 INFO train_south.py line 277 27002] Epoch: [2/150][3/52] Data 0.001 (17.241) Batch 1.630 (18.855) Remain 40:33:51 Loss 1.0987 Accuracy 0.3336.
[2022-09-03 18:45:06,247 INFO train_south.py line 277 27002] Epoch: [2/150][4/52] Data 0.002 (12.931) Batch 1.565 (14.533) Remain 31:15:40 Loss 1.0986 Accuracy 0.3340.
[2022-09-03 18:45:07,806 INFO train_south.py line 277 27002] Epoch: [2/150][5/52] Data 0.001 (10.345) Batch 1.559 (11.938) Remain 25:40:34 Loss 1.0987 Accuracy 0.3345.
[2022-09-03 18:45:12,346 INFO train_south.py line 277 27002] Epoch: [2/150][6/52] Data 2.969 (9.116) Batch 4.540 (10.705) Remain 23:01:16 Loss 1.0987 Accuracy 0.3333.
[2022-09-03 18:45:14,014 INFO train_south.py line 277 27002] Epoch: [2/150][7/52] Data 0.002 (7.814) Batch 1.668 (9.414) Remain 20:14:33 Loss 1.0986 Accuracy 0.3347.
[2022-09-03 18:45:15,567 INFO train_south.py line 277 27002] Epoch: [2/150][8/52] Data 0.001 (6.837) Batch 1.553 (8.431) Remain 18:07:37 Loss 1.0987 Accuracy 0.3326.
[2022-09-03 18:45:17,120 INFO train_south.py line 277 27002] Epoch: [2/150][9/52] Data 0.002 (6.078) Batch 1.553 (7.667) Remain 16:28:54 Loss 1.0986 Accuracy 0.3347.
[2022-09-03 18:45:18,678 INFO train_south.py line 277 27002] Epoch: [2/150][10/52] Data 0.001 (5.470) Batch 1.559 (7.056) Remain 15:10:00 Loss 1.0986 Accuracy 0.3334.
[2022-09-03 18:45:20,231 INFO train_south.py line 277 27002] Epoch: [2/150][11/52] Data 0.001 (4.973) Batch 1.553 (6.556) Remain 14:05:22 Loss 1.0986 Accuracy 0.3338.
[2022-09-03 18:45:21,800 INFO train_south.py line 277 27002] Epoch: [2/150][12/52] Data 0.001 (4.559) Batch 1.569 (6.140) Remain 13:11:41 Loss 1.0987 Accuracy 0.3326.
[2022-09-03 18:45:23,356 INFO train_south.py line 277 27002] Epoch: [2/150][13/52] Data 0.001 (4.208) Batch 1.555 (5.788) Remain 12:26:07 Loss 1.0986 Accuracy 0.3334.
[2022-09-03 18:45:24,910 INFO train_south.py line 277 27002] Epoch: [2/150][14/52] Data 0.002 (3.908) Batch 1.554 (5.485) Remain 11:47:02 Loss 1.0987 Accuracy 0.3336.
[2022-09-03 18:45:26,468 INFO train_south.py line 277 27002] Epoch: [2/150][15/52] Data 0.002 (3.647) Batch 1.558 (5.223) Remain 11:13:12 Loss 1.0987 Accuracy 0.3330.
[2022-09-03 18:45:28,028 INFO train_south.py line 277 27002] Epoch: [2/150][16/52] Data 0.002 (3.420) Batch 1.560 (4.994) Remain 10:43:36 Loss 1.0987 Accuracy 0.3327.
[2022-09-03 18:45:29,583 INFO train_south.py line 277 27002] Epoch: [2/150][17/52] Data 0.001 (3.218) Batch 1.555 (4.792) Remain 10:17:27 Loss 1.0986 Accuracy 0.3330.
[2022-09-03 18:45:31,158 INFO train_south.py line 277 27002] Epoch: [2/150][18/52] Data 0.001 (3.040) Batch 1.575 (4.613) Remain 09:54:21 Loss 1.0986 Accuracy 0.3349.
[2022-09-03 18:45:32,716 INFO train_south.py line 277 27002] Epoch: [2/150][19/52] Data 0.001 (2.880) Batch 1.558 (4.453) Remain 09:33:34 Loss 1.0987 Accuracy 0.3333.
[2022-09-03 18:45:34,278 INFO train_south.py line 277 27002] Epoch: [2/150][20/52] Data 0.001 (2.736) Batch 1.561 (4.308) Remain 09:14:52 Loss 1.0986 Accuracy 0.3339.
[2022-09-03 18:45:35,840 INFO train_south.py line 277 27002] Epoch: [2/150][21/52] Data 0.001 (2.606) Batch 1.563 (4.177) Remain 08:57:58 Loss 1.0987 Accuracy 0.3340.
[2022-09-03 18:45:51,575 INFO train_south.py line 277 27002] Epoch: [2/150][22/52] Data 14.154 (3.131) Batch 15.735 (4.703) Remain 10:05:32 Loss 1.0987 Accuracy 0.3337.
[2022-09-03 18:45:53,130 INFO train_south.py line 277 27002] Epoch: [2/150][23/52] Data 0.002 (2.995) Batch 1.554 (4.566) Remain 09:47:50 Loss 1.0987 Accuracy 0.3334.
[2022-09-03 18:45:54,681 INFO train_south.py line 277 27002] Epoch: [2/150][24/52] Data 0.002 (2.870) Batch 1.552 (4.440) Remain 09:31:35 Loss 1.0987 Accuracy 0.3333.
[2022-09-03 18:45:56,233 INFO train_south.py line 277 27002] Epoch: [2/150][25/52] Data 0.001 (2.755) Batch 1.552 (4.325) Remain 09:16:39 Loss 1.0987 Accuracy 0.3337.
[2022-09-03 18:45:57,783 INFO train_south.py line 277 27002] Epoch: [2/150][26/52] Data 0.001 (2.649) Batch 1.550 (4.218) Remain 09:02:50 Loss 1.0987 Accuracy 0.3332.
[2022-09-03 18:45:59,334 INFO train_south.py line 277 27002] Epoch: [2/150][27/52] Data 0.001 (2.551) Batch 1.551 (4.119) Remain 08:50:03 Loss 1.0986 Accuracy 0.3332.
[2022-09-03 18:46:00,896 INFO train_south.py line 277 27002] Epoch: [2/150][28/52] Data 0.001 (2.460) Batch 1.562 (4.028) Remain 08:38:14 Loss 1.0987 Accuracy 0.3340.
[2022-09-03 18:46:02,452 INFO train_south.py line 277 27002] Epoch: [2/150][29/52] Data 0.001 (2.375) Batch 1.556 (3.943) Remain 08:27:12 Loss 1.0987 Accuracy 0.3332.
[2022-09-03 18:46:04,005 INFO train_south.py line 277 27002] Epoch: [2/150][30/52] Data 0.001 (2.296) Batch 1.553 (3.863) Remain 08:16:54 Loss 1.0986 Accuracy 0.3342.
[2022-09-03 18:46:05,560 INFO train_south.py line 277 27002] Epoch: [2/150][31/52] Data 0.001 (2.222) Batch 1.555 (3.789) Remain 08:07:15 Loss 1.0986 Accuracy 0.3339.
[2022-09-03 18:46:07,121 INFO train_south.py line 277 27002] Epoch: [2/150][32/52] Data 0.002 (2.153) Batch 1.560 (3.719) Remain 07:58:14 Loss 1.0987 Accuracy 0.3340.
[2022-09-03 18:46:08,695 INFO train_south.py line 277 27002] Epoch: [2/150][33/52] Data 0.003 (2.088) Batch 1.574 (3.654) Remain 07:49:49 Loss 1.0987 Accuracy 0.3342.
[2022-09-03 18:46:10,254 INFO train_south.py line 277 27002] Epoch: [2/150][34/52] Data 0.002 (2.026) Batch 1.559 (3.592) Remain 07:41:50 Loss 1.0987 Accuracy 0.3328.
[2022-09-03 18:46:11,811 INFO train_south.py line 277 27002] Epoch: [2/150][35/52] Data 0.001 (1.968) Batch 1.557 (3.534) Remain 07:34:18 Loss 1.0986 Accuracy 0.3335.
[2022-09-03 18:46:13,370 INFO train_south.py line 277 27002] Epoch: [2/150][36/52] Data 0.001 (1.914) Batch 1.559 (3.479) Remain 07:27:12 Loss 1.0987 Accuracy 0.3327.
[2022-09-03 18:46:14,928 INFO train_south.py line 277 27002] Epoch: [2/150][37/52] Data 0.002 (1.862) Batch 1.557 (3.427) Remain 07:20:28 Loss 1.0986 Accuracy 0.3336.
[2022-09-03 18:46:16,486 INFO train_south.py line 277 27002] Epoch: [2/150][38/52] Data 0.001 (1.813) Batch 1.559 (3.378) Remain 07:14:05 Loss 1.0986 Accuracy 0.3336.
[2022-09-03 18:46:18,045 INFO train_south.py line 277 27002] Epoch: [2/150][39/52] Data 0.001 (1.767) Batch 1.559 (3.332) Remain 07:08:02 Loss 1.0986 Accuracy 0.3350.
[2022-09-03 18:46:19,610 INFO train_south.py line 277 27002] Epoch: [2/150][40/52] Data 0.001 (1.722) Batch 1.565 (3.287) Remain 07:02:18 Loss 1.0986 Accuracy 0.3330.
[2022-09-03 18:46:21,167 INFO train_south.py line 277 27002] Epoch: [2/150][41/52] Data 0.001 (1.680) Batch 1.557 (3.245) Remain 06:56:50 Loss 1.0986 Accuracy 0.3343.
[2022-09-03 18:46:22,726 INFO train_south.py line 277 27002] Epoch: [2/150][42/52] Data 0.001 (1.641) Batch 1.559 (3.205) Remain 06:51:37 Loss 1.0986 Accuracy 0.3324.
[2022-09-03 18:46:24,283 INFO train_south.py line 277 27002] Epoch: [2/150][43/52] Data 0.001 (1.602) Batch 1.557 (3.167) Remain 06:46:39 Loss 1.0986 Accuracy 0.3334.
[2022-09-03 18:46:25,842 INFO train_south.py line 277 27002] Epoch: [2/150][44/52] Data 0.001 (1.566) Batch 1.559 (3.130) Remain 06:41:54 Loss 1.0986 Accuracy 0.3334.
[2022-09-03 18:46:27,399 INFO train_south.py line 277 27002] Epoch: [2/150][45/52] Data 0.001 (1.531) Batch 1.558 (3.095) Remain 06:37:22 Loss 1.0987 Accuracy 0.3327.
[2022-09-03 18:46:28,956 INFO train_south.py line 277 27002] Epoch: [2/150][46/52] Data 0.001 (1.498) Batch 1.556 (3.062) Remain 06:33:01 Loss 1.0986 Accuracy 0.3331.
[2022-09-03 18:46:30,511 INFO train_south.py line 277 27002] Epoch: [2/150][47/52] Data 0.001 (1.466) Batch 1.555 (3.030) Remain 06:28:51 Loss 1.0987 Accuracy 0.3334.
[2022-09-03 18:46:32,067 INFO train_south.py line 277 27002] Epoch: [2/150][48/52] Data 0.001 (1.436) Batch 1.556 (2.999) Remain 06:24:52 Loss 1.0986 Accuracy 0.3337.
[2022-09-03 18:46:33,624 INFO train_south.py line 277 27002] Epoch: [2/150][49/52] Data 0.001 (1.406) Batch 1.558 (2.970) Remain 06:21:02 Loss 1.0986 Accuracy 0.3347.
[2022-09-03 18:46:35,181 INFO train_south.py line 277 27002] Epoch: [2/150][50/52] Data 0.001 (1.378) Batch 1.557 (2.941) Remain 06:17:22 Loss 1.0986 Accuracy 0.3338.
[2022-09-03 18:46:36,748 INFO train_south.py line 277 27002] Epoch: [2/150][51/52] Data 0.001 (1.351) Batch 1.566 (2.914) Remain 06:13:51 Loss 1.0986 Accuracy 0.3340.
[2022-09-03 18:46:38,316 INFO train_south.py line 277 27002] Epoch: [2/150][52/52] Data 0.001 (1.325) Batch 1.569 (2.888) Remain 06:10:29 Loss 1.0986 Accuracy 0.3332.
[2022-09-03 18:46:38,568 INFO train_south.py line 299 27002] Train result at epoch [2/150]: mIoU/mAcc/allAcc 0.1988/0.3335/0.3336.
[2022-09-03 18:46:38,570 INFO train_south.py line 305 27002] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[2022-09-03 18:47:31,623 INFO train_south.py line 344 27002] Test: [1/1] Data 13.687 (13.687) Batch 53.044 (53.044) Loss 1.0986 (1.0986) Accuracy 0.3334.
[2022-09-03 18:47:31,833 INFO train_south.py line 361 27002] Val result: mIoU/mAcc/allAcc 0.1685/0.3333/0.3334.
[2022-09-03 18:47:31,833 INFO train_south.py line 363 27002] Class_0 Result: iou/accuracy 0.1547/0.2241.
[2022-09-03 18:47:31,833 INFO train_south.py line 363 27002] Class_1 Result: iou/accuracy 0.0571/0.0644.
[2022-09-03 18:47:31,833 INFO train_south.py line 363 27002] Class_2 Result: iou/accuracy 0.2937/0.7115.
[2022-09-03 18:47:31,833 INFO train_south.py line 364 27002] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2022-09-03 18:47:31,834 INFO train_south.py line 215 27002] Saving checkpoint to: exp/south/pointtransformer_repro/model/model_last.pth
[2022-09-03 18:47:32,081 INFO train_south.py line 219 27002] Best validation mIoU updated to: 0.1685
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
